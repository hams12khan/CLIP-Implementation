{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc2aaf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "This script requires PyTorch version 1.8.0 or higher.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m__version__ \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.8.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis script requires PyTorch version 1.8.0 or higher.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: This script requires PyTorch version 1.8.0 or higher."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.__version__ >= \"4.8.0\", \"This script requires PyTorch version 1.8.0 or higher.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c39da",
   "metadata": {},
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f9bfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e69dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the output image features: torch.Size([10, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ff = FeedForward(dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attention(x, x, x)[0])\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers=nn.ModuleList([\n",
    "                TransformerBlock(dim, heads, mlp_dim, dropout)\n",
    "\n",
    "            ])\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (class token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)\n",
    "\n",
    "# Example Usage:\n",
    "# This demonstrates how to instantiate and use the Vision Transformer model.\n",
    "\n",
    "# Configuration for a 'base' ViT model\n",
    "image_encoder = ViT(\n",
    "    image_size = 224,\n",
    "    patch_size = 32,\n",
    "    num_classes = 768,  # This is the embedding dimension, not the number of classes for classification\n",
    "    dim = 768,\n",
    "    depth = 12,\n",
    "    heads = 12,\n",
    "    mlp_dim = 3072,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_encoder.to(device)\n",
    "\n",
    "\n",
    "# Create a dummy image tensor\n",
    "dummy_image = torch.randn(10, 3, 224, 224)\n",
    "dummy_image=dummy_image.to(device)\n",
    "\n",
    "# Get the image embeddings\n",
    "image_features = image_encoder(dummy_image)\n",
    "\n",
    "print(\"Shape of the output image features:\", image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386a0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = torch.tensor([\n",
    "  [1, 2, 3, 4, 5],\n",
    "  [6, 7, 8, 0, 0] \n",
    "])\n",
    "\n",
    "mask= (sequences== 0).unsqueeze(1).unsqueeze(2)  # Create a mask for padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fdaf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "tensor([[[[False, False, False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False, False, False,  True,  True]]]])\n",
      "Shape of the mask: torch.Size([2, 1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(sequences.shape)\n",
    "print(mask)\n",
    "print(\"Shape of the mask:\", mask.shape)  # [Batcch, 1, 1, Sequence Length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b70b0ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 10079232\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in image_encoder.parameters())\n",
    "print(\"Total parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e8960",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b3a3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_encoder import ViT\n",
    "\n",
    "image_encoder =ViT(\n",
    "    image_size = 224,\n",
    "    patch_size = 16,\n",
    "    num_classes = 512,  # This is the embedding dimension, not the number of classes for classification\n",
    "    dim = 768,\n",
    "    layer = 12,\n",
    "    heads = 12,\n",
    "    mlp_dim = 3072,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2024e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8225792\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "parser=argparse.ArgumentParser(description=\"I am Biggner with argparse\")\n",
    "\n",
    "parser.add_argument('number1', type=int, default=12, help='first number to add')\n",
    "parser.add_argument(\"number2\",type=int,default=13,help=\"second number to add\")\n",
    "parser.add_argument(\"operations\",type=str, choices=['add', 'subtract', 'multiply', 'divide'], help=\"operation to perform\")\n",
    "\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4accb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hammad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
